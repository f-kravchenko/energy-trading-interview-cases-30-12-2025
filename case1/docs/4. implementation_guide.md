# Implementation Guide - Heat Pump Steering System

## Executive Summary

This guide provides a practical, step-by-step plan to migrate from the current heat pump steering implementation to the improved architecture. The implementation is structured in 3 phases over 6 months, with each phase delivering incremental value and maintaining backward compatibility.

## Technology Stack

### Core Technologies

| Component | Technology | Rationale |
|-----------|-----------|-----------|
| **API Framework** | FastAPI (Python 3.11+) | High performance, async support, automatic OpenAPI docs |
| **Workflow Orchestration** | Temporal.io | Durable workflows, built-in retries, excellent observability |
| **Database (Transactional)** | PostgreSQL 15 | ACID compliance, JSON support, mature ecosystem |
| **Database (Real-time)** | Cloud Firestore | Real-time sync, low-latency reads, serverless |
| **Analytics Database** | ClickHouse | Columnar storage, SQL interface, real-time analytics, open-source |
| **Cache & Queue** | Redis 7 | Low latency, pub/sub, distributed locks |
| **Message Broker** | Cloud Pub/Sub | Managed, scalable, at-least-once delivery |
| **Optimization** | CVXPY + Gurobi | Convex optimization, handles MPC formulation |
| **Container Orchestration** | GKE (Kubernetes) | Industry standard, auto-scaling, managed service |
| **Monitoring** | Prometheus + Grafana | Rich metrics, flexible dashboards, alerting |
| **Logging** | Cloud Logging (structured JSON) | Centralized, searchable, retention policies |
| **Tracing** | OpenTelemetry + Cloud Trace | Distributed tracing, vendor-agnostic |
| **IaC** | Terraform | Declarative, version controlled, state management |
| **CI/CD** | GitHub Actions | Integrated with repo, easy to use, free tier |

### Python Dependencies

```toml
# pyproject.toml
[tool.poetry.dependencies]
python = "^3.11"

# API & Web
fastapi = "^0.104.0"
uvicorn = {extras = ["standard"], version = "^0.24.0"}
pydantic = "^2.5.0"
httpx = "^0.25.0"

# Workflow orchestration
temporalio = "^1.5.0"

# Database & ORM
sqlalchemy = "^2.0.0"
asyncpg = "^0.29.0"
alembic = "^1.12.0"

# Google Cloud
google-cloud-firestore = "^2.13.0"
google-cloud-pubsub = "^2.18.0"
clickhouse-connect = "^0.9.2"
google-cloud-secret-manager = "^2.17.0"

# Caching
redis = {extras = ["hiredis"], version = "^5.0.0"}

# Optimization
cvxpy = "^1.4.0"
numpy = "^1.26.0"
scipy = "^1.11.0"
pandas = "^2.1.0"

# Circuit breaker & resilience
pybreaker = "^1.0.0"
tenacity = "^8.2.0"

# Monitoring
prometheus-client = "^0.19.0"
opentelemetry-api = "^1.21.0"
opentelemetry-sdk = "^1.21.0"
opentelemetry-exporter-gcp-trace = "^1.6.0"

# Testing
pytest = "^7.4.0"
pytest-asyncio = "^0.21.0"
pytest-cov = "^4.1.0"
factory-boy = "^3.3.0"
faker = "^20.0.0"

# Code quality
ruff = "^0.1.0"
mypy = "^1.7.0"
black = "^23.11.0"
```

## Project Structure

```
heat-pump-steering/
├── .github/
│   └── workflows/
│       ├── ci.yml                    # Run tests, linting
│       └── deploy.yml                # Deploy to GCP
├── infrastructure/
│   ├── terraform/
│   │   ├── main.tf                   # GCP infrastructure
│   │   ├── variables.tf
│   │   └── outputs.tf
│   └── kubernetes/
│       ├── deployments/
│       │   ├── optimization-engine.yaml
│       │   ├── api-gateway.yaml
│       │   └── reconciliation-service.yaml
│       └── services/
├── src/
│   ├── adapters/                     # OEM adapter implementations
│   │   ├── __init__.py
│   │   ├── base.py                   # OEMAdapter abstract class
│   │   ├── brand_a.py
│   │   ├── brand_b.py
│   │   └── registry.py
│   ├── api/                          # FastAPI application
│   │   ├── __init__.py
│   │   ├── main.py
│   │   ├── routes/
│   │   │   ├── devices.py
│   │   │   ├── commands.py
│   │   │   └── optimization.py
│   │   └── models/                   # Pydantic models
│   ├── workflows/                    # Temporal workflows
│   │   ├── __init__.py
│   │   ├── command_execution.py
│   │   └── reconciliation.py
│   ├── optimization/                 # Optimization algorithms
│   │   ├── __init__.py
│   │   ├── mpc.py                    # Model Predictive Control
│   │   ├── thermal_model.py
│   │   └── forecasting.py
│   ├── services/                     # Business logic
│   │   ├── __init__.py
│   │   ├── command_service.py
│   │   ├── device_service.py
│   │   └── pricing_service.py
│   ├── database/                     # Database layer
│   │   ├── __init__.py
│   │   ├── models.py                 # SQLAlchemy models
│   │   ├── repositories/
│   │   └── migrations/
│   ├── observability/                # Monitoring, logging, tracing
│   │   ├── __init__.py
│   │   ├── metrics.py
│   │   ├── tracing.py
│   │   └── logging.py
│   └── config/                       # Configuration
│       ├── __init__.py
│       └── settings.py
├── tests/
│   ├── unit/
│   ├── integration/
│   └── e2e/
├── scripts/
│   ├── seed_test_data.py
│   └── migrate_from_celery.py
├── docs/
│   ├── api.md
│   ├── architecture.md
│   └── runbooks/
├── pyproject.toml
├── Dockerfile
└── README.md
```

## Implementation Phases

### Phase 1: Foundation & Migration (Weeks 1-8)

**Goal:** Establish new infrastructure while maintaining current system

#### Week 1-2: Infrastructure Setup

**Tasks:**

1. **Set up GCP project and basic infrastructure**

```bash
# Create GCP project
gcloud projects create heat-pump-steering-prod --name="Heat Pump Steering"

# Enable required APIs
gcloud services enable \
  container.googleapis.com \
  sql-component.googleapis.com \
  pubsub.googleapis.com \
  secretmanager.googleapis.com \
  logging.googleapis.com \
  monitoring.googleapis.com
```

2. **Deploy PostgreSQL instance**

```hcl
# infrastructure/terraform/database.tf
resource "google_sql_database_instance" "main" {
  name             = "heat-pump-db"
  database_version = "POSTGRES_15"
  region           = var.region

  settings {
    tier = "db-custom-2-7680"  # 2 vCPU, 7.68 GB RAM

    backup_configuration {
      enabled                        = true
      point_in_time_recovery_enabled = true
      start_time                     = "03:00"
    }

    ip_configuration {
      ipv4_enabled    = false
      private_network = google_compute_network.vpc.id
    }

    database_flags {
      name  = "max_connections"
      value = "200"
    }
  }
}

resource "google_sql_database" "database" {
  name     = "heat_pump_steering"
  instance = google_sql_database_instance.main.name
}
```

3. **Deploy GKE cluster**

```hcl
# infrastructure/terraform/gke.tf
resource "google_container_cluster" "primary" {
  name     = "heat-pump-cluster"
  location = var.region

  # Autopilot for automatic management
  enable_autopilot = true

  # Network
  network    = google_compute_network.vpc.name
  subnetwork = google_compute_subnetwork.subnet.name

  # Security
  workload_identity_config {
    workload_pool = "${var.project_id}.svc.id.goog"
  }

  # Monitoring
  monitoring_config {
    enable_components = ["SYSTEM_COMPONENTS", "WORKLOADS"]
    managed_prometheus {
      enabled = true
    }
  }
}
```

4. **Set up Pub/Sub topics**

```hcl
# infrastructure/terraform/pubsub.tf
resource "google_pubsub_topic" "spot_prices" {
  name = "spot-price-updates"
}

resource "google_pubsub_topic" "device_telemetry" {
  name = "device-telemetry"
}

resource "google_pubsub_topic" "command_events" {
  name = "command-events"
}

resource "google_pubsub_subscription" "optimization_trigger" {
  name  = "optimization-trigger-sub"
  topic = google_pubsub_topic.spot_prices.name

  ack_deadline_seconds = 60

  retry_policy {
    minimum_backoff = "10s"
    maximum_backoff = "600s"
  }
}
```

#### Week 3-4: Core Data Models & Database

**Tasks:**

1. **Define SQLAlchemy models**

```python
# src/database/models.py
from sqlalchemy import Column, String, Integer, Float, DateTime, Enum, JSON, ForeignKey
from sqlalchemy.orm import relationship
from sqlalchemy.ext.declarative import declarative_base
from datetime import datetime
import enum

Base = declarative_base()

class CommandState(enum.Enum):
    PENDING = "pending"
    SENT = "sent"
    ACKNOWLEDGED = "acknowledged"
    VERIFIED = "verified"
    COMPLETED = "completed"
    FAILED = "failed"

class Device(Base):
    __tablename__ = "devices"

    id = Column(String, primary_key=True)
    user_id = Column(String, ForeignKey("users.id"), nullable=False)
    oem_name = Column(String, nullable=False)
    model = Column(String)
    location_lat = Column(Float)
    location_lon = Column(Float)

    # Thermal parameters (learned from data)
    thermal_resistance = Column(Float)  # K/W
    thermal_capacity = Column(Float)    # Wh/K
    cop = Column(Float)                 # Coefficient of performance

    # Capabilities
    min_power_kw = Column(Float, default=0.0)
    max_power_kw = Column(Float, default=5.0)

    # Metadata
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

    # Relationships
    commands = relationship("Command", back_populates="device")
    user = relationship("User", back_populates="devices")


class Command(Base):
    __tablename__ = "commands"

    id = Column(String, primary_key=True)
    device_id = Column(String, ForeignKey("devices.id"), nullable=False)
    correlation_id = Column(String, index=True)  # For tracing

    # Command details
    generic_action = Column(String, nullable=False)  # e.g., "set_mode_max"
    oem_specific_payload = Column(JSON)

    # State machine
    state = Column(Enum(CommandState), default=CommandState.PENDING)
    retry_count = Column(Integer, default=0)

    # Timestamps
    created_at = Column(DateTime, default=datetime.utcnow)
    sent_at = Column(DateTime, nullable=True)
    acknowledged_at = Column(DateTime, nullable=True)
    verified_at = Column(DateTime, nullable=True)
    completed_at = Column(DateTime, nullable=True)

    # Results
    success = Column(Boolean, nullable=True)
    error_message = Column(String, nullable=True)
    response_data = Column(JSON)

    # Relationships
    device = relationship("Device", back_populates="commands")


class User(Base):
    __tablename__ = "users"

    id = Column(String, primary_key=True)
    email = Column(String, unique=True, nullable=False)

    # Preferences
    preferences = Column(JSON, default={
        "min_temp": 18.0,
        "max_temp": 24.0,
        "target_temp": 21.0,
        "cost_weight": 1.0,
        "comfort_weight": 1.0
    })

    created_at = Column(DateTime, default=datetime.utcnow)

    # Relationships
    devices = relationship("Device", back_populates="user")
```

2. **Create database migrations**

```bash
# Initialize Alembic
alembic init src/database/migrations

# Create initial migration
alembic revision --autogenerate -m "Initial schema"

# Apply migrations
alembic upgrade head
```

3. **Implement repositories**

```python
# src/database/repositories/command_repository.py
from typing import List, Optional
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, update
from datetime import datetime

from ..models import Command, CommandState

class CommandRepository:
    def __init__(self, session: AsyncSession):
        self.session = session

    async def create(self, command: Command) -> Command:
        self.session.add(command)
        await self.session.commit()
        await self.session.refresh(command)
        return command

    async def get_by_id(self, command_id: str) -> Optional[Command]:
        result = await self.session.execute(
            select(Command).where(Command.id == command_id)
        )
        return result.scalar_one_or_none()

    async def update_state(
        self,
        command_id: str,
        new_state: CommandState,
        error_message: Optional[str] = None
    ) -> None:
        timestamp_field = f"{new_state.value}_at"

        await self.session.execute(
            update(Command)
            .where(Command.id == command_id)
            .values(
                state=new_state,
                **{timestamp_field: datetime.utcnow()},
                error_message=error_message
            )
        )
        await self.session.commit()

    async def get_pending_commands(self, limit: int = 100) -> List[Command]:
        result = await self.session.execute(
            select(Command)
            .where(Command.state == CommandState.PENDING)
            .order_by(Command.created_at)
            .limit(limit)
        )
        return result.scalars().all()
```

#### Week 5-6: Adapter Pattern Implementation

**Tasks:**

1. **Implement base adapter interface**

```python
# src/adapters/base.py
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Dict, Any, Optional
from datetime import datetime

@dataclass
class CommandResult:
    success: bool
    response_data: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    timestamp: datetime = datetime.utcnow()

@dataclass
class DeviceState:
    temperature_indoor: float
    temperature_outdoor: Optional[float]
    mode: str
    power_kw: float
    timestamp: datetime

@dataclass
class RateLimitConfig:
    max_requests_per_minute: int
    max_requests_per_hour: int
    max_commands_per_day: int

class OEMAdapter(ABC):
    """Base class for all OEM adapters"""

    @abstractmethod
    async def send_command(
        self,
        device_id: str,
        generic_action: str,
        **kwargs
    ) -> CommandResult:
        """Send command to device via OEM API"""
        pass

    @abstractmethod
    async def get_device_state(self, device_id: str) -> DeviceState:
        """Query current device state"""
        pass

    @abstractmethod
    def translate_generic_command(
        self,
        generic_action: str
    ) -> Dict[str, Any]:
        """Translate generic action to OEM-specific payload"""
        pass

    @property
    @abstractmethod
    def rate_limits(self) -> RateLimitConfig:
        """OEM-specific rate limits"""
        pass
```

2. **Implement concrete adapters**

```python
# src/adapters/brand_a.py
import httpx
from typing import Dict, Any
from pybreaker import CircuitBreaker

from .base import OEMAdapter, CommandResult, DeviceState, RateLimitConfig

class BrandAAdapter(OEMAdapter):
    """Adapter for BrandA heat pumps"""

    def __init__(
        self,
        api_base_url: str,
        api_key: str,
        circuit_breaker: CircuitBreaker
    ):
        self.api_base_url = api_base_url
        self.api_key = api_key
        self.circuit_breaker = circuit_breaker
        self.client = httpx.AsyncClient(
            base_url=api_base_url,
            headers={"Authorization": f"Bearer {api_key}"},
            timeout=30.0
        )

    async def send_command(
        self,
        device_id: str,
        generic_action: str,
        **kwargs
    ) -> CommandResult:
        payload = self.translate_generic_command(generic_action)

        try:
            # Use circuit breaker for protection
            response = await self.circuit_breaker.call_async(
                self._post_command,
                device_id,
                payload
            )

            return CommandResult(
                success=True,
                response_data=response
            )
        except Exception as e:
            return CommandResult(
                success=False,
                error=str(e)
            )

    async def _post_command(
        self,
        device_id: str,
        payload: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Internal method for actual API call"""
        response = await self.client.post(
            f"/devices/{device_id}/commands",
            json=payload
        )
        response.raise_for_status()
        return response.json()

    async def get_device_state(self, device_id: str) -> DeviceState:
        response = await self.client.get(f"/devices/{device_id}/state")
        response.raise_for_status()
        data = response.json()

        return DeviceState(
            temperature_indoor=data["indoor_temp"],
            temperature_outdoor=data.get("outdoor_temp"),
            mode=data["mode"],
            power_kw=data["power_kw"],
            timestamp=datetime.fromisoformat(data["timestamp"])
        )

    def translate_generic_command(self, generic_action: str) -> Dict[str, Any]:
        """Map generic actions to BrandA API calls"""
        mapping = {
            "set_mode_max": {
                "temperature_offset": 2,
                "boost_dhw": True,
                "eco_mode": False
            },
            "set_mode_min": {
                "temperature_offset": -2,
                "boost_dhw": False,
                "eco_mode": True
            },
            "set_mode_mid": {
                "temperature_offset": 0,
                "boost_dhw": False,
                "eco_mode": False
            }
        }
        return mapping.get(generic_action, {})

    @property
    def rate_limits(self) -> RateLimitConfig:
        return RateLimitConfig(
            max_requests_per_minute=10,
            max_requests_per_hour=200,
            max_commands_per_day=500
        )
```

3. **Create adapter registry**

```python
# src/adapters/registry.py
from typing import Dict, Optional
from .base import OEMAdapter

class AdapterRegistry:
    """Central registry for OEM adapters"""

    def __init__(self):
        self._adapters: Dict[str, OEMAdapter] = {}

    def register(self, oem_name: str, adapter: OEMAdapter) -> None:
        self._adapters[oem_name] = adapter

    def get(self, oem_name: str) -> Optional[OEMAdapter]:
        return self._adapters.get(oem_name)

    def list_oems(self) -> list[str]:
        return list(self._adapters.keys())

# Global instance
adapter_registry = AdapterRegistry()
```

#### Week 7-8: Temporal Workflow Implementation

**Tasks:**

1. **Deploy Temporal server**

```yaml
# infrastructure/kubernetes/deployments/temporal.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: temporal-server
spec:
  replicas: 1
  template:
    spec:
      containers:
      - name: temporal
        image: temporalio/auto-setup:latest
        env:
        - name: DB
          value: postgresql
        - name: DB_PORT
          value: "5432"
        - name: POSTGRES_USER
          valueFrom:
            secretKeyRef:
              name: postgres-credentials
              key: username
        - name: POSTGRES_PWD
          valueFrom:
            secretKeyRef:
              name: postgres-credentials
              key: password
```

2. **Implement command execution workflow**

```python
# src/workflows/command_execution.py
from temporalio import workflow, activity
from datetime import timedelta
from typing import Optional

from ..database.models import Command, CommandState
from ..adapters.registry import adapter_registry

@workflow.defn
class CommandExecutionWorkflow:
    """
    Orchestrate full lifecycle of a command
    """

    def __init__(self):
        self.command_acknowledged = False

    @workflow.run
    async def run(self, command_id: str) -> bool:
        # 1. Load command details
        command = await workflow.execute_activity(
            load_command,
            command_id,
            start_to_close_timeout=timedelta(seconds=10)
        )

        # 2. Send command with retry
        send_result = await workflow.execute_activity(
            send_command_to_device,
            command,
            start_to_close_timeout=timedelta(seconds=60),
            retry_policy=workflow.RetryPolicy(
                initial_interval=timedelta(seconds=5),
                maximum_interval=timedelta(minutes=5),
                maximum_attempts=5,
                backoff_coefficient=2.0
            )
        )

        if not send_result:
            return False

        # 3. Wait for acknowledgment (with timeout)
        try:
            await workflow.wait_condition(
                lambda: self.command_acknowledged,
                timeout=timedelta(minutes=2)
            )
        except TimeoutError:
            workflow.logger.warning(f"Command {command_id} not acknowledged")
            # Continue anyway - verification will catch issues

        # 4. Schedule verification
        await workflow.sleep(timedelta(minutes=10))

        verified = await workflow.execute_activity(
            verify_command_execution,
            command,
            start_to_close_timeout=timedelta(seconds=30)
        )

        return verified

    @workflow.signal
    def acknowledge_command(self) -> None:
        """Signal from device that command was received"""
        self.command_acknowledged = True


@activity.defn
async def load_command(command_id: str) -> Command:
    """Load command from database"""
    # Implementation
    pass

@activity.defn
async def send_command_to_device(command: Command) -> bool:
    """Send command via OEM adapter"""
    adapter = adapter_registry.get(command.device.oem_name)

    result = await adapter.send_command(
        device_id=command.device_id,
        generic_action=command.generic_action
    )

    # Update command state
    if result.success:
        await update_command_state(command.id, CommandState.SENT)
        return True
    else:
        await update_command_state(
            command.id,
            CommandState.FAILED,
            error_message=result.error
        )
        return False

@activity.defn
async def verify_command_execution(command: Command) -> bool:
    """Verify command was executed by checking device state"""
    adapter = adapter_registry.get(command.device.oem_name)

    device_state = await adapter.get_device_state(command.device_id)

    # Check if state matches expected outcome
    expected = compute_expected_state(command)

    if states_match(device_state, expected):
        await update_command_state(command.id, CommandState.VERIFIED)
        return True
    else:
        workflow.logger.warning(
            f"State mismatch for command {command.id}: "
            f"expected {expected}, got {device_state}"
        )
        return False
```

### Phase 2: Optimization Engine (Weeks 9-16)

#### Week 9-10: Thermal Model Implementation

**Tasks:**

1. **Implement RC thermal model**

```python
# src/optimization/thermal_model.py
import numpy as np
from scipy.optimize import least_squares
from typing import Tuple
from dataclasses import dataclass

@dataclass
class ThermalParameters:
    R: float  # Thermal resistance (K/W)
    C: float  # Thermal capacity (Wh/K)
    COP: float  # Coefficient of performance

class ThermalModel:
    """Building thermal model for optimization"""

    def simulate(
        self,
        T_initial: float,
        T_outdoor: np.ndarray,
        P_heating: np.ndarray,
        params: ThermalParameters,
        dt: float = 0.25  # hours
    ) -> np.ndarray:
        """
        Simulate indoor temperature evolution

        Args:
            T_initial: Initial indoor temperature (°C)
            T_outdoor: Outdoor temperature array (°C)
            P_heating: Heating power array (kW)
            params: Thermal parameters
            dt: Time step (hours)

        Returns:
            Array of indoor temperatures
        """
        T_indoor = [T_initial]

        for i in range(len(P_heating)):
            heat_loss = (T_indoor[-1] - T_outdoor[i]) / params.R
            heat_gain = P_heating[i] * params.COP

            dT = (heat_gain - heat_loss) / params.C * dt
            T_indoor.append(T_indoor[-1] + dT)

        return np.array(T_indoor)

    def identify_parameters(
        self,
        T_indoor_measured: np.ndarray,
        T_outdoor_measured: np.ndarray,
        P_heating_measured: np.ndarray,
        dt: float = 0.25
    ) -> ThermalParameters:
        """
        Identify building thermal parameters from data

        Uses least squares to fit RC model to measured data
        """

        def residual(params):
            R, C, COP = params

            T_simulated = self.simulate(
                T_indoor_measured[0],
                T_outdoor_measured,
                P_heating_measured,
                ThermalParameters(R=R, C=C, COP=COP),
                dt
            )

            return T_simulated[:-1] - T_indoor_measured

        # Initial guess and bounds
        x0 = [0.01, 1000, 3.0]
        bounds = ([0.001, 100, 1.5], [0.1, 10000, 5.0])

        result = least_squares(residual, x0, bounds=bounds)

        return ThermalParameters(
            R=result.x[0],
            C=result.x[1],
            COP=result.x[2]
        )
```

2. **Create parameter learning service**

```python
# src/services/thermal_learning_service.py
from datetime import datetime, timedelta
from typing import Optional

from ..optimization.thermal_model import ThermalModel, ThermalParameters
from ..database.repositories import DeviceRepository

class ThermalLearningService:
    """Learn building thermal parameters from historical data"""

    def __init__(self, device_repo: DeviceRepository):
        self.device_repo = device_repo
        self.model = ThermalModel()

    async def learn_parameters(
        self,
        device_id: str,
        lookback_days: int = 14
    ) -> Optional[ThermalParameters]:
        """
        Learn thermal parameters for a device

        Uses last N days of telemetry data
        """
        # Fetch historical telemetry
        end_time = datetime.utcnow()
        start_time = end_time - timedelta(days=lookback_days)

        telemetry = await self.fetch_telemetry(
            device_id,
            start_time,
            end_time
        )

        if len(telemetry) < 100:
            return None  # Not enough data

        # Extract arrays
        T_indoor = np.array([t.temperature_indoor for t in telemetry])
        T_outdoor = np.array([t.temperature_outdoor for t in telemetry])
        P_heating = np.array([t.power_kw for t in telemetry])

        # Identify parameters
        params = self.model.identify_parameters(
            T_indoor, T_outdoor, P_heating
        )

        # Store in database
        await self.device_repo.update_thermal_params(device_id, params)

        return params
```

#### Week 11-12: MPC Optimizer

**Tasks:**

1. **Implement MPC optimizer**

```python
# src/optimization/mpc.py
import cvxpy as cp
import numpy as np
from typing import Dict, List
from dataclasses import dataclass

from .thermal_model import ThermalParameters

@dataclass
class UserPreferences:
    min_temp: float = 18.0
    max_temp: float = 24.0
    target_temp: float = 21.0
    cost_weight: float = 1.0
    comfort_weight: float = 1.0
    wear_weight: float = 0.1

class MPCOptimizer:
    """Model Predictive Control optimizer"""

    def __init__(
        self,
        horizon_hours: int = 24,
        timestep_minutes: int = 15
    ):
        self.horizon = horizon_hours * 60 // timestep_minutes
        self.dt = timestep_minutes / 60  # hours

    def optimize(
        self,
        current_temp: float,
        prices: np.ndarray,
        outdoor_temp: np.ndarray,
        thermal_params: ThermalParameters,
        device_constraints: Dict,
        user_prefs: UserPreferences
    ) -> np.ndarray:
        """
        Optimize heating schedule

        Returns: Array of heating power setpoints (kW)
        """
        n = len(prices)

        # Decision variables
        T_inside = cp.Variable(n + 1)
        P_heat = cp.Variable(n)

        # Constraints
        constraints = []

        # Initial condition
        constraints.append(T_inside[0] == current_temp)

        # Thermal dynamics
        for i in range(n):
            heat_loss = (T_inside[i] - outdoor_temp[i]) / thermal_params.R
            heat_gain = P_heat[i] * thermal_params.COP

            T_next = T_inside[i] + (self.dt / thermal_params.C) * (heat_gain - heat_loss)
            constraints.append(T_inside[i + 1] == T_next)

        # Comfort bounds
        constraints.append(T_inside[1:] >= user_prefs.min_temp)
        constraints.append(T_inside[1:] <= user_prefs.max_temp)

        # Power limits
        constraints.append(P_heat >= device_constraints['min_power_kw'])
        constraints.append(P_heat <= device_constraints['max_power_kw'])

        # Objective
        energy_cost = cp.sum(cp.multiply(prices, P_heat) * self.dt)
        comfort_cost = cp.sum_squares(T_inside[1:] - user_prefs.target_temp)
        wear_cost = cp.sum(cp.abs(cp.diff(P_heat)))

        objective = cp.Minimize(
            user_prefs.cost_weight * energy_cost +
            user_prefs.comfort_weight * comfort_cost +
            user_prefs.wear_weight * wear_cost
        )

        # Solve
        problem = cp.Problem(objective, constraints)
        problem.solve(solver=cp.ECOS)

        if problem.status != cp.OPTIMAL:
            raise ValueError(f"Optimization failed: {problem.status}")

        return P_heat.value
```

2. **Create optimization service**

```python
# src/services/optimization_service.py
from datetime import datetime, timedelta
from typing import List

from ..optimization.mpc import MPCOptimizer, UserPreferences
from ..services.pricing_service import PricingService
from ..services.weather_service import WeatherService
from ..database.repositories import DeviceRepository

class OptimizationService:
    """Orchestrate optimization for devices"""

    def __init__(
        self,
        device_repo: DeviceRepository,
        pricing_service: PricingService,
        weather_service: WeatherService
    ):
        self.device_repo = device_repo
        self.pricing_service = pricing_service
        self.weather_service = weather_service
        self.optimizer = MPCOptimizer(horizon_hours=24)

    async def optimize_device(self, device_id: str) -> List[float]:
        """
        Run optimization for a single device

        Returns: Heating power schedule for next 24 hours
        """
        # Load device details
        device = await self.device_repo.get_by_id(device_id)

        # Get current state
        current_state = await self.get_current_state(device_id)

        # Get forecasts
        prices = await self.pricing_service.get_forecast(
            start=datetime.utcnow(),
            hours=24
        )

        outdoor_temps = await self.weather_service.get_temperature_forecast(
            lat=device.location_lat,
            lon=device.location_lon,
            hours=24
        )

        # Run optimization
        schedule = self.optimizer.optimize(
            current_temp=current_state.temperature_indoor,
            prices=np.array(prices),
            outdoor_temp=np.array(outdoor_temps),
            thermal_params=ThermalParameters(
                R=device.thermal_resistance,
                C=device.thermal_capacity,
                COP=device.cop
            ),
            device_constraints={
                'min_power_kw': device.min_power_kw,
                'max_power_kw': device.max_power_kw
            },
            user_prefs=UserPreferences(**device.user.preferences)
        )

        return schedule.tolist()
```

### Phase 3: Observability & Grid Services (Weeks 17-24)

#### Week 17-18: Monitoring & Alerting

**Tasks:**

1. **Set up Prometheus metrics**

```python
# src/observability/metrics.py
from prometheus_client import Counter, Histogram, Gauge

# Command metrics
command_sent_total = Counter(
    'commands_sent_total',
    'Total commands sent',
    ['oem', 'device_id']
)

command_duration_seconds = Histogram(
    'command_duration_seconds',
    'Command execution duration',
    ['oem']
)

# Optimization metrics
optimization_runtime_seconds = Histogram(
    'optimization_runtime_seconds',
    'Optimization algorithm runtime'
)

estimated_cost_savings_eur = Gauge(
    'estimated_cost_savings_eur',
    'Estimated cost savings',
    ['device_id']
)
```

2. **Deploy Grafana dashboards**

```json
// grafana/dashboards/command_execution.json
{
  "dashboard": {
    "title": "Command Execution",
    "panels": [
      {
        "title": "Command Success Rate",
        "targets": [{
          "expr": "rate(commands_sent_total{success='true'}[5m]) / rate(commands_sent_total[5m])"
        }]
      },
      {
        "title": "Command Latency (p95)",
        "targets": [{
          "expr": "histogram_quantile(0.95, rate(command_duration_seconds_bucket[5m]))"
        }]
      }
    ]
  }
}
```

#### Week 19-24: Grid Services Integration

Implementation of VPP aggregation and grid service participation (see economic_optimization.md for details).

## Migration Strategy

### Parallel Run Phase

**Duration:** 4 weeks

Run old (Celery) and new (Temporal) systems in parallel:

1. **Shadow mode:** New system observes but doesn't send commands
2. **Compare outputs:** Validate that both systems produce similar results
3. **Gradual cutover:** Migrate 10% → 50% → 100% of devices

```python
# src/services/migration_service.py
class MigrationService:
    """Manage gradual migration from Celery to Temporal"""

    async def should_use_new_system(self, device_id: str) -> bool:
        """
        Determine if device should use new system

        Gradually roll out based on device_id hash
        """
        rollout_percentage = await self.get_rollout_percentage()

        device_hash = int(hashlib.md5(device_id.encode()).hexdigest(), 16)
        device_bucket = device_hash % 100

        return device_bucket < rollout_percentage
```

## Testing Strategy

### Unit Tests

```python
# tests/unit/test_mpc_optimizer.py
import pytest
import numpy as np

from src.optimization.mpc import MPCOptimizer, UserPreferences
from src.optimization.thermal_model import ThermalParameters

def test_mpc_optimization_basic():
    """Test MPC produces valid schedule"""
    optimizer = MPCOptimizer(horizon_hours=24)

    # Simple scenario: constant price, constant outdoor temp
    prices = np.ones(96) * 0.20  # €0.20/kWh
    outdoor_temp = np.ones(96) * 5.0  # 5°C

    schedule = optimizer.optimize(
        current_temp=20.0,
        prices=prices,
        outdoor_temp=outdoor_temp,
        thermal_params=ThermalParameters(R=0.01, C=1000, COP=3.0),
        device_constraints={'min_power_kw': 0, 'max_power_kw': 5},
        user_prefs=UserPreferences()
    )

    # Validate output
    assert len(schedule) == 96
    assert np.all(schedule >= 0)
    assert np.all(schedule <= 5)

def test_mpc_responds_to_price_changes():
    """Test MPC reduces heating during high prices"""
    optimizer = MPCOptimizer(horizon_hours=24)

    # High price in middle of horizon
    prices = np.ones(96) * 0.20
    prices[48:52] = 1.00  # Price spike

    outdoor_temp = np.ones(96) * 5.0

    schedule = optimizer.optimize(
        current_temp=20.0,
        prices=prices,
        outdoor_temp=outdoor_temp,
        thermal_params=ThermalParameters(R=0.01, C=1000, COP=3.0),
        device_constraints={'min_power_kw': 0, 'max_power_kw': 5},
        user_prefs=UserPreferences()
    )

    # Should reduce heating during price spike
    avg_power_normal = np.mean(schedule[:48])
    avg_power_spike = np.mean(schedule[48:52])

    assert avg_power_spike < avg_power_normal
```

### Integration Tests

```python
# tests/integration/test_command_workflow.py
import pytest
from temporalio.testing import WorkflowEnvironment

from src.workflows.command_execution import CommandExecutionWorkflow

@pytest.mark.asyncio
async def test_command_workflow_success():
    """Test successful command execution workflow"""
    async with await WorkflowEnvironment.start_local() as env:
        async with WorkflowClient(
            env.get_client_async()
        ) as client:
            result = await client.execute_workflow(
                CommandExecutionWorkflow.run,
                "test_command_123",
                id="test-workflow",
                task_queue="test-queue"
            )

            assert result == True
```

## Rollback Plan

If critical issues arise during migration:

1. **Immediate:** Switch traffic back to Celery (feature flag)
2. **Data consistency:** Ensure Celery can read new database schema
3. **Command replay:** Re-execute failed commands from event log

```python
# Rollback feature flag
ENABLE_TEMPORAL_WORKFLOWS = os.getenv("ENABLE_TEMPORAL", "false").lower() == "true"

if ENABLE_TEMPORAL_WORKFLOWS:
    await execute_temporal_workflow(command)
else:
    celery_task.delay(command.id)  # Fallback to Celery
```

## Success Metrics

Track these KPIs to measure implementation success:

| Metric | Baseline | Target | Measurement |
|--------|----------|--------|-------------|
| Command success rate | 85% | 99% | `commands_sent_total{success="true"} / commands_sent_total` |
| P95 command latency | 30s | 5s | `histogram_quantile(0.95, command_duration_seconds)` |
| State drift incidents | 50/day | 5/day | Manual review + alerts |
| Cost savings per household | €0 | €420/year | ClickHouse analytics |
| System uptime | 95% | 99.9% | Cloud Monitoring |

## Conclusion

This implementation guide provides a practical path to migrate from the current system to the improved architecture over 6 months. Key takeaways:

- **Phase 1 (Weeks 1-8):** Build foundation with infrastructure, data models, adapters
- **Phase 2 (Weeks 9-16):** Implement MPC optimization and thermal modeling
- **Phase 3 (Weeks 17-24):** Add observability and grid services

Each phase delivers incremental value while maintaining backward compatibility. The parallel run strategy ensures safe migration with rollback capability.

Related documents:
- [1. heat_pump_steering_analysis.md](1.%20heat_pump_steering_analysis.md) - Problem analysis
- [2. improved_architecture.md](2.%20improved_architecture.md) - Architecture design
- [3. economic_optimization.md](3.%20economic_optimization.md) - Optimization strategies
